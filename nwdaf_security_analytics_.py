# -*- coding: utf-8 -*-
"""NWDAF Security Analytics .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O8TfUmJ5LWAnTnDU0jljrJq0em2uPH5C
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install flwr[simulation] ray pandas scikit-learn torch torchvision opacus psutil

# ============================================================================
# IMPORTS
# ============================================================================

import os
import sys
import json
import time
import pickle
import psutil  # Make sure to pip install psutil
import warnings
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from collections import defaultdict

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# ML libraries
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report
)

# PyTorch
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# Federated Learning
import flwr as fl
from flwr.common import Metrics, NDArrays, Scalar
from flwr.common import ndarrays_to_parameters, parameters_to_ndarrays
# --- FIX: Only import FedAvg, as we will define the others manually ---
from flwr.server.strategy import FedAvg

# Differential Privacy
from opacus import PrivacyEngine
from opacus.validators import ModuleValidator

# Parallelization
import ray  # Make sure to pip install ray

warnings.filterwarnings('ignore')
sns.set_style('whitegrid')


# ============================================================================
# CONFIGURATION
# ============================================================================

class ExperimentConfig:
    """Centralized configuration for reproducibility"""

    # Paths
    DATA_PATH = '/content/drive/MyDrive/IDSDatasets/UNSW 15' # Update this
    RESULTS_DIR = './results'
    LOGS_DIR = './logs'

    # Random seeds for reproducibility
    RANDOM_SEED = 42

    # Dataset parameters
    TEST_SIZE = 0.2
    VAL_SIZE = 0.1  # From training set

    # Federated Learning parameters
    NUM_CLIENTS = 5
    NUM_ROUNDS = 50
    CLIENT_FRACTION = 1.0
    MIN_FIT_CLIENTS = 5
    MIN_AVAILABLE_CLIENTS = 5

    # Local training parameters
    LOCAL_EPOCHS = 1
    BATCH_SIZE = 64
    LEARNING_RATE = 0.001

    # Differential Privacy parameters
    EPSILON_VALUES = [0.5, 1.0, 3.0, 5.0, 10.0, float('inf')]  # inf = no DP
    TARGET_DELTA = 1e-5
    MAX_GRAD_NORM = 1.0

    # Robustness parameters
    MALICIOUS_FRACTIONS = [0.0, 0.1, 0.2, 0.3]
    ATTACK_TYPES = ['none', 'label_flip', 'sign_flip', 'random_noise']
    AGGREGATORS = ['fedavg', 'trimmed_mean', 'median', 'krum']
    TRIM_RATIO = 0.1  # For trimmed mean
    KRUM_TO_KEEP = 1 # For Krum, number of clients to keep (k)

    # Model parameters
    HIDDEN_DIMS = [128, 64, 32]
    DROPOUT_RATE = 0.2

    # Experiment parameters
    NUM_RUNS = 3  # Multiple runs for statistical significance

    # Device
    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    @classmethod
    def setup_directories(cls):
        """Create necessary directories"""
        Path(cls.RESULTS_DIR).mkdir(parents=True, exist_ok=True)
        Path(cls.LOGS_DIR).mkdir(parents=True, exist_ok=True)

    @classmethod
    def set_seed(cls, seed: int = None):
        """Set random seeds for reproducibility"""
        seed = seed or cls.RANDOM_SEED
        np.random.seed(seed)
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)

config = ExperimentConfig()
config.setup_directories()
config.set_seed()

# ============================================================================
# DATA LOADING AND PREPROCESSING
# ============================================================================

class UNSWDataLoader:
    """Handle UNSW-NB15 dataset loading and preprocessing"""

    def __init__(self, data_path: str):
        self.data_path = data_path
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()

    def load_and_preprocess(self) -> Tuple[np.ndarray, np.ndarray, List[str]]:
        """Load UNSW-NB15 dataset"""
        print("=" * 80)
        print("LOADING UNSW-NB15 DATASET")
        print("=" * 80)

        # Try different file patterns
        possible_files = [
            'UNSW-NB15.csv',
            'UNSW_NB15_training-set.csv',
            'UNSW_NB15_testing-set.csv'
        ]

        df = None
        for filename in possible_files:
            filepath = os.path.join(self.data_path, filename)
            if os.path.exists(filepath):
                print(f"âœ“ Found: {filepath}")
                df = pd.read_csv(filepath, low_memory=False)
                break

        if df is None:
            # Try loading multiple files
            train_path = os.path.join(self.data_path, 'UNSW_NB15_training-set.csv')
            test_path = os.path.join(self.data_path, 'UNSW_NB15_testing-set.csv')

            if os.path.exists(train_path) and os.path.exists(test_path):
                print(f"âœ“ Loading training and testing sets separately")
                df_train = pd.read_csv(train_path, low_memory=False)
                df_test = pd.read_csv(test_path, low_memory=False)
                df = pd.concat([df_train, df_test], ignore_index=True)
            else:
                raise FileNotFoundError(f"Could not find UNSW-NB15 dataset in {self.data_path}")

        print(f"âœ“ Dataset loaded: {df.shape[0]} samples, {df.shape[1]} features")

        # Display basic info
        print(f"\nColumns: {df.columns.tolist()[:10]}... (showing first 10)")

        # Handle label column (different naming conventions)
        label_col = 'label'  # Direct assignment

        print(f"âœ“ Using label column: '{label_col}'")

        # Create binary labels
        y = df[label_col].values

        # Remove non-feature columns
        drop_cols = ['id', 'attack_cat', 'label']
        # Drop columns that exist, ignore errors if they don't
        X = df.drop(columns=drop_cols, errors='ignore')

        # Handle categorical features
        categorical_cols = ['proto', 'service', 'state']
        for col in categorical_cols:
            if col in X.columns:
                le = LabelEncoder()
                X[col] = le.fit_transform(X[col].astype(str))

        # Convert to numpy
        feature_names = list(X.columns)

        # Handle missing/infinite values
        # Convert all to numeric, coercing errors to NaN
        X = X.apply(pd.to_numeric, errors='coerce')
        X = np.nan_to_num(X.values, nan=0.0, posinf=1e10, neginf=-1e10)

        # Class distribution
        unique, counts = np.unique(y, return_counts=True)
        print(f"\nâœ“ Class distribution:")
        for label, count in zip(unique, counts):
            print(f"  Class {label}: {count:,} ({count/len(y)*100:.2f}%)")

        print(f"\nâœ“ Final feature matrix: {X.shape}")
        print(f"âœ“ Label vector: {y.shape}")

        return X, y, feature_names

# ============================================================================
# FEDERATED DATA PARTITIONING
# ============================================================================

class FederatedDataPartitioner:
    """Partition data across clients with different strategies"""

    @staticmethod
    def iid_partition(X: np.ndarray, y: np.ndarray, num_clients: int) -> List[Tuple]:
        """IID partitioning - random split"""
        indices = np.random.permutation(len(X))
        splits = np.array_split(indices, num_clients)

        partitions = []
        for split in splits:
            partitions.append((X[split], y[split]))

        return partitions

    @staticmethod
    def non_iid_partition(X: np.ndarray, y: np.ndarray, num_clients: int,
                            alpha: float = 0.5) -> List[Tuple]:
        """Non-IID partitioning using Dirichlet distribution"""
        num_classes = len(np.unique(y))
        client_data = [[] for _ in range(num_clients)]

        for class_idx in range(num_classes):
            class_indices = np.where(y == class_idx)[0]
            np.random.shuffle(class_indices)

            # Dirichlet distribution
            proportions = np.random.dirichlet(alpha=np.repeat(alpha, num_clients))
            proportions = (np.cumsum(proportions) * len(class_indices)).astype(int)[:-1]

            splits = np.split(class_indices, proportions)
            for client_idx, split in enumerate(splits):
                client_data[client_idx].extend(split)

        partitions = []
        for indices in client_data:
            indices = np.array(indices)
            np.random.shuffle(indices)
            partitions.append((X[indices], y[indices]))

        return partitions

# ============================================================================
# NEURAL NETWORK MODEL
# ============================================================================

class IntrusionDetectionMLP(nn.Module):
    """MLP for binary intrusion detection"""

    def __init__(self, input_dim: int, hidden_dims: List[int] = [128, 64, 32],
                 dropout: float = 0.2):
        super().__init__()

        layers = []
        prev_dim = input_dim

        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.GroupNorm(num_groups=min(32, hidden_dim), num_channels=hidden_dim),  # DP-compatible
                nn.ReLU(),
                nn.Dropout(dropout)
            ])
            prev_dim = hidden_dim

        layers.append(nn.Linear(prev_dim, 1))
        layers.append(nn.Sigmoid())

        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)

# ============================================================================
# METRICS TRACKER
# ============================================================================

class MetricsTracker:
    """Track all experimental metrics"""

    def __init__(self):
        self.metrics = defaultdict(list)

    def log(self, **kwargs):
        """Log metrics"""
        for key, value in kwargs.items():
            self.metrics[key].append(value)

    def get_summary(self) -> Dict:
        """Get summary statistics"""
        summary = {}
        for key, values in self.metrics.items():
            if isinstance(values[0], (int, float)):
                summary[key] = {
                    'mean': np.mean(values),
                    'std': np.std(values),
                    'min': np.min(values),
                    'max': np.max(values)
                }
        return summary

    def save(self, filepath: str):
        """Save metrics to file"""
        with open(filepath, 'w') as f:
            json.dump(dict(self.metrics), f, indent=2, default=str)

# ============================================================================
# FEDERATED LEARNING CLIENTS
# ============================================================================

class BaseClient(fl.client.NumPyClient):
    """Base client with DP and metrics tracking"""

    def __init__(self, cid: int, model: nn.Module, trainloader: DataLoader,
                 testloader: DataLoader, config: ExperimentConfig):
        self.cid = cid
        self.model = model
        self.trainloader = trainloader
        self.testloader = testloader
        self.config = config
        self.criterion = nn.BCELoss()
        self.optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE)
        self.privacy_engine = None
        self.metrics_tracker = MetricsTracker()

    def setup_dp(self, epsilon: float):
        """Setup differential privacy"""
        if epsilon == float('inf'):
            return  # No DP

        self.privacy_engine = PrivacyEngine()

        # Use make_private_with_epsilon to automatically calculate noise
        self.model, self.optimizer, self.trainloader = self.privacy_engine.make_private_with_epsilon(
            module=self.model,
            optimizer=self.optimizer,
            data_loader=self.trainloader,
            target_epsilon=epsilon,
            target_delta=self.config.TARGET_DELTA,
            epochs=self.config.LOCAL_EPOCHS,
            max_grad_norm=self.config.MAX_GRAD_NORM,
        )

    def get_parameters(self, config: Dict) -> NDArrays:
        return [val.cpu().numpy() for _, val in self.model.state_dict().items()]

    def set_parameters(self, parameters: NDArrays):
        params_dict = zip(self.model.state_dict().keys(), parameters)
        state_dict = {k: torch.tensor(v) for k, v in params_dict}
        self.model.load_state_dict(state_dict, strict=True)

    def fit(self, parameters: NDArrays, config: Dict) -> Tuple[NDArrays, int, Dict]:
        """Train the model"""
        self.set_parameters(parameters)
        self.model.train()

        # Operational metrics
        start_time = time.time()
        start_cpu = psutil.Process().cpu_times()

        total_loss = 0
        for epoch in range(self.config.LOCAL_EPOCHS):
            for batch_idx, (data, target) in enumerate(self.trainloader):
                data, target = data.to(self.config.DEVICE), target.to(self.config.DEVICE)

                self.optimizer.zero_grad()
                output = self.model(data)
                loss = self.criterion(output, target)
                loss.backward()
                self.optimizer.step()

                total_loss += loss.item()

        # Calculate metrics
        end_time = time.time()
        end_cpu = psutil.Process().cpu_times()

        updated_params = self.get_parameters(config={})
        param_size = sum(p.nbytes for p in updated_params)

        metrics = {
            'client_id': self.cid,
            'latency': end_time - start_time,
            'cpu_time': end_cpu.user - start_cpu.user,
            'bandwidth_bytes': param_size,
            'avg_loss': total_loss / len(self.trainloader) if len(self.trainloader) > 0 else 0
        }

        # Privacy metrics
        if self.privacy_engine:
            epsilon = self.privacy_engine.get_epsilon(delta=self.config.TARGET_DELTA)
            metrics['epsilon_spent'] = epsilon
        else:
            metrics['epsilon_spent'] = float('inf')

        return updated_params, len(self.trainloader.dataset), metrics

    def evaluate(self, parameters: NDArrays, config: Dict) -> Tuple[float, int, Dict]:
        """Evaluate the model"""
        self.set_parameters(parameters)
        self.model.eval()

        all_preds = []
        all_labels = []
        all_probs = []
        total_loss = 0

        with torch.no_grad():
            for data, target in self.testloader:
                data, target = data.to(self.config.DEVICE), target.to(self.config.DEVICE)
                output = self.model(data)
                loss = self.criterion(output, target)
                total_loss += loss.item()

                probs = output.cpu().numpy()
                preds = (output > 0.5).float().cpu().numpy()

                all_probs.extend(probs.flatten())
                all_preds.extend(preds.flatten())
                all_labels.extend(target.cpu().numpy().flatten())

        # Calculate metrics
        accuracy = accuracy_score(all_labels, all_preds)
        precision = precision_score(all_labels, all_preds, zero_division=0)
        recall = recall_score(all_labels, all_preds, zero_division=0)
        f1 = f1_score(all_labels, all_preds, zero_division=0)

        try:
            auc = roc_auc_score(all_labels, all_probs)
        except:
            auc = 0.5

        metrics = {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'auc': auc,
            'loss': total_loss / len(self.testloader) if len(self.testloader) > 0 else 0
        }

        return float(total_loss / len(self.testloader)) if len(self.testloader) > 0 else 0.0, len(self.testloader.dataset), metrics


class MaliciousClient(BaseClient):
    """Client that performs attacks"""

    def __init__(self, cid: int, model: nn.Module, trainloader: DataLoader,
                 testloader: DataLoader, config: ExperimentConfig, attack_type: str):
        super().__init__(cid, model, trainloader, testloader, config)
        self.attack_type = attack_type
        print(f"âš ï¸  Client {cid} is MALICIOUS (Attack: {attack_type})")

    def fit(self, parameters: NDArrays, config: Dict) -> Tuple[NDArrays, int, Dict]:
        """Train with attack"""

        if self.attack_type == 'label_flip':
            return self._label_flip_attack(parameters, config)
        elif self.attack_type == 'sign_flip':
            return self._sign_flip_attack(parameters, config)
        elif self.attack_type == 'random_noise':
            return self._random_noise_attack(parameters, config)
        else:
            return super().fit(parameters, config)

    def _label_flip_attack(self, parameters: NDArrays, config: Dict):
        """Flip all labels"""
        self.set_parameters(parameters)
        self.model.train()

        start_time = time.time()

        for epoch in range(self.config.LOCAL_EPOCHS):
            for data, target in self.trainloader:
                # Flip labels
                flipped_target = 1 - target

                data = data.to(self.config.DEVICE)
                flipped_target = flipped_target.to(self.config.DEVICE)

                self.optimizer.zero_grad()
                output = self.model(data)
                loss = self.criterion(output, flipped_target)
                loss.backward()
                self.optimizer.step()

        latency = time.time() - start_time
        updated_params = self.get_parameters(config={})

        metrics = {
            'client_id': self.cid,
            'latency': latency,
            'attack_type': 'label_flip'
        }

        return updated_params, len(self.trainloader.dataset), metrics

    def _sign_flip_attack(self, parameters: NDArrays, config: Dict):
        """Flip gradient signs"""
        _, num_samples, metrics = super().fit(parameters, config)

        updated_params = self.get_parameters(config={})
        flipped_params = [-3.0 * p for p in updated_params]

        metrics['attack_type'] = 'sign_flip'
        return flipped_params, num_samples, metrics

    def _random_noise_attack(self, parameters: NDArrays, config: Dict):
        """Send random noise"""
        start_time = time.time()

        # Generate random parameters
        random_params = [np.random.randn(*p.shape).astype(np.float32) for p in parameters]

        latency = time.time() - start_time
        metrics = {
            'client_id': self.cid,
            'latency': latency,
            'attack_type': 'random_noise'
        }

        return random_params, len(self.trainloader.dataset), metrics


# ============================================================================
# ROBUST AGGREGATION STRATEGIES (FIX: ADDING CUSTOM CLASSES BACK)
# ============================================================================

class TrimmedMeanStrategy(FedAvg):
    """Custom Trimmed mean aggregation strategy"""

    def __init__(self, trim_ratio: float = 0.1, **kwargs):
        super().__init__(**kwargs)
        self.trim_ratio = trim_ratio

    def aggregate_fit(
        self,
        server_round: int,
        results: List[Tuple[fl.server.client_proxy.ClientProxy, fl.common.FitRes]],
        failures: List[BaseException],
    ) -> Tuple[Optional[fl.common.Parameters], Dict[str, Scalar]]:

        if not results:
            return None, {}

        # Convert results to weights
        weights_results = [
            (parameters_to_ndarrays(fit_res.parameters), fit_res.num_examples)
            for _, fit_res in results
        ]

        # --- This is the slow, coordinate-wise trimmed mean ---
        weights = [w for w, _ in weights_results]
        aggregated = []
        for layer_idx in range(len(weights[0])):
            layer_weights = np.stack([w[layer_idx] for w in weights])
            trim_count = int(self.trim_ratio * len(weights))

            if trim_count > 0:
                sorted_weights = np.sort(layer_weights, axis=0)
                trimmed = sorted_weights[trim_count:-trim_count]
                aggregated_layer = np.mean(trimmed, axis=0)
            else:
                aggregated_layer = np.mean(layer_weights, axis=0)

            aggregated.append(aggregated_layer)
        # --- End of slow block ---

        parameters_aggregated = ndarrays_to_parameters(aggregated)

        # Aggregate metrics
        metrics_aggregated = {}
        if self.fit_metrics_aggregation_fn:
            fit_metrics = [(res.num_examples, res.metrics) for _, res in results]
            metrics_aggregated = self.fit_metrics_aggregation_fn(fit_metrics)

        return parameters_aggregated, metrics_aggregated


class MedianStrategy(FedAvg):
    """Custom Coordinate-wise median aggregation"""

    def aggregate_fit(
        self,
        server_round: int,
        results: List[Tuple[fl.server.client_proxy.ClientProxy, fl.common.FitRes]],
        failures: List[BaseException],
    ) -> Tuple[Optional[fl.common.Parameters], Dict[str, Scalar]]:

        if not results:
            return None, {}

        weights_results = [
            (parameters_to_ndarrays(fit_res.parameters), fit_res.num_examples)
            for _, fit_res in results
        ]

        # --- This is the slow, coordinate-wise median ---
        weights = [w for w, _ in weights_results]
        aggregated = []
        for layer_idx in range(len(weights[0])):
            layer_weights = np.stack([w[layer_idx] for w in weights])
            aggregated_layer = np.median(layer_weights, axis=0)
            aggregated.append(aggregated_layer)
        # --- End of slow block ---

        parameters_aggregated = ndarrays_to_parameters(aggregated)

        # Aggregate metrics
        metrics_aggregated = {}
        if self.fit_metrics_aggregation_fn:
            fit_metrics = [(res.num_examples, res.metrics) for _, res in results]
            metrics_aggregated = self.fit_metrics_aggregation_fn(fit_metrics)

        return parameters_aggregated, metrics_aggregated


class KrumStrategy(FedAvg):
    """Custom Krum aggregation strategy"""

    def __init__(self, to_keep: int, **kwargs):
        super().__init__(**kwargs)
        self.to_keep = to_keep

    def aggregate_fit(
        self,
        server_round: int,
        results: List[Tuple[fl.server.client_proxy.ClientProxy, fl.common.FitRes]],
        failures: List[BaseException],
    ) -> Tuple[Optional[fl.common.Parameters], Dict[str, Scalar]]:

        if not results:
            return None, {}

        weights_results = [
            (parameters_to_ndarrays(fit_res.parameters), fit_res.num_examples)
            for _, fit_res in results
        ]

        weights = [w for w, _ in weights_results]
        num_clients = len(weights)

        if num_clients < self.to_keep:
            # Not enough clients, fall back to FedAvg
            return super().aggregate_fit(server_round, results, failures)

        # --- Krum Calculation (Slow) ---
        # Flatten weights for distance calculation
        flat_weights = [np.concatenate([l.flatten() for l in w]) for w in weights]
        scores = []

        for i in range(num_clients):
            dists = []
            for j in range(num_clients):
                if i == j:
                    continue
                dist = np.linalg.norm(flat_weights[i] - flat_weights[j]) ** 2
                dists.append(dist)

            # Get sum of squared distances to k-2 nearest neighbors
            dists.sort()
            # This logic might need tuning based on the krum variant
            # We select the n-f-2 closest clients, where f is num_malicious
            num_malicious = num_clients - self.to_keep
            num_neighbors = max(1, num_clients - num_malicious - 2)
            scores.append(sum(dists[:num_neighbors]))

        # Get the indices of the clients with the lowest scores
        best_indices = np.argsort(scores)[:self.to_keep]

        # Aggregate the weights of the best clients
        best_weights = [weights[i] for i in best_indices]

        aggregated = []
        for layer_idx in range(len(best_weights[0])):
            layer_weights = np.stack([w[layer_idx] for w in best_weights])
            aggregated_layer = np.mean(layer_weights, axis=0)
            aggregated.append(aggregated_layer)
        # --- End of Krum block ---

        parameters_aggregated = ndarrays_to_parameters(aggregated)

        # Aggregate metrics
        metrics_aggregated = {}
        if self.fit_metrics_aggregation_fn:
            fit_metrics = [(res.num_examples, res.metrics) for _, res in results]
            metrics_aggregated = self.fit_metrics_aggregation_fn(fit_metrics)

        return parameters_aggregated, metrics_aggregated


# ============================================================================
# EXPERIMENT RUNNER (PARALLELIZED WITH RAY)
# ============================================================================

class ExperimentRunner:
    """Run complete experimental grid IN PARALLEL using Ray"""

    def __init__(self, config: ExperimentConfig):
        self.config = config
        self.results = defaultdict(list)

    @staticmethod
    @ray.remote
    def _run_single_experiment(X_train, y_train, X_test, y_test, input_dim,
                               epsilon, alpha, attack_type, aggregator, run_id,
                               base_config):
        """Run a single experiment configuration (Ray-compatible)"""

        # --- 1. Set seed for this specific run ---
        ExperimentConfig.set_seed(base_config.RANDOM_SEED + run_id)

        # --- 2. Partition data ---
        partitions = FederatedDataPartitioner.iid_partition(
            X_train, y_train, base_config.NUM_CLIENTS
        )

        # --- 3. Determine malicious clients ---
        num_malicious = int(alpha * base_config.NUM_CLIENTS)
        malicious_ids = list(range(num_malicious))

        # --- 4. Define client_fn ---
        def client_fn(cid: str):
            client_id = int(cid)
            X_client, y_client = partitions[client_id]

            # Create dataloaders
            train_dataset = TensorDataset(
                torch.FloatTensor(X_client),
                torch.FloatTensor(y_client).view(-1, 1)
            )
            train_loader = DataLoader(train_dataset, batch_size=base_config.BATCH_SIZE, shuffle=True)

            test_dataset = TensorDataset(
                torch.FloatTensor(X_test),
                torch.FloatTensor(y_test).view(-1, 1)
            )
            test_loader = DataLoader(test_dataset, batch_size=base_config.BATCH_SIZE)

            # Create model
            model = IntrusionDetectionMLP(
                input_dim=input_dim,
                hidden_dims=base_config.HIDDEN_DIMS,
                dropout=base_config.DROPOUT_RATE
            ).to(base_config.DEVICE)

            # Create client
            if client_id in malicious_ids and attack_type != 'none':
                return MaliciousClient(client_id, model, train_loader, test_loader,
                                       base_config, attack_type).to_client()
            else:
                client = BaseClient(client_id, model, train_loader, test_loader, base_config)
                if epsilon != float('inf'):
                    client.setup_dp(epsilon) # setup_dp calls make_private_with_epsilon
                return client.to_client()

        # --- 5. Select strategy (FIX: Use custom classes) ---
        def weighted_avg(metrics: List[Tuple[int, Metrics]]) -> Metrics:
            accuracies = [m['accuracy'] * n for n, m in metrics]
            f1s = [m['f1'] * n for n, m in metrics]
            aucs = [m['auc'] * n for n, m in metrics]
            total = sum(n for n, _ in metrics)
            if total == 0: return {}
            return {
                'accuracy': sum(accuracies) / total,
                'f1': sum(f1s) / total,
                'auc': sum(aucs) / total
            }

        strategy_params = {
            "fraction_fit": base_config.CLIENT_FRACTION,
            "fraction_evaluate": 1.0,
            "min_fit_clients": base_config.MIN_FIT_CLIENTS,
            "min_available_clients": base_config.MIN_AVAILABLE_CLIENTS,
            "evaluate_metrics_aggregation_fn": weighted_avg,
            "fit_metrics_aggregation_fn": weighted_avg, # Also aggregate fit metrics
        }

        if aggregator == 'trimmed_mean':
            strategy = TrimmedMeanStrategy(
                **strategy_params,
                trim_ratio=base_config.TRIM_RATIO
            )
        elif aggregator == 'median':
            strategy = MedianStrategy(
                **strategy_params
            )
        elif aggregator == 'krum':
            # Calculate k: number of honest clients to keep
            k = max(1, base_config.NUM_CLIENTS - num_malicious)
            # Krum's `to_keep` is often just the number of honest clients
            strategy = KrumStrategy(
                **strategy_params,
                to_keep=k
            )
        else: # fedavg
            strategy = FedAvg(
                **strategy_params
            )

        # --- 6. Run simulation ---
        start_exp_time = time.time()

        history = fl.simulation.start_simulation(
            client_fn=client_fn,
            num_clients=base_config.NUM_CLIENTS,
            config=fl.server.ServerConfig(num_rounds=base_config.NUM_ROUNDS),
            strategy=strategy,
            client_resources={"num_cpus": 1, "num_gpus": 0.0}
        )

        total_exp_time = time.time() - start_exp_time

        # --- 7. Extract results ---
        final_metrics = {}

        # Get client-side metrics (latency, cpu)
        if history.metrics_centralized and 'fit_metrics' in history.metrics_centralized:
            # metrics_centralized is a dict: {round_num: (list_of_client_metrics)}
            # We need to flatten this list of lists
            all_fit_metrics = [m for _, m_list in history.metrics_centralized['fit_metrics'] for _, m in m_list]
            all_latencies = [m['latency'] for m in all_fit_metrics if 'latency' in m]
            all_cpu_times = [m['cpu_time'] for m in all_fit_metrics if 'cpu_time' in m]
            final_metrics['avg_client_latency'] = np.mean(all_latencies) if all_latencies else 0.0
            final_metrics['avg_client_cpu'] = np.mean(all_cpu_times) if all_cpu_times else 0.0

        # Get server-side evaluation metrics
        if history.metrics_distributed:
            for metric_name in ['accuracy', 'f1', 'auc']:
                if metric_name in history.metrics_distributed:
                    values = [v for _, v in history.metrics_distributed[metric_name]]
                    final_metrics[f'final_{metric_name}'] = values[-1] if values else 0.0
                    final_metrics[f'best_{metric_name}'] = max(values) if values else 0.0
                    final_metrics[f'avg_{metric_name}'] = np.mean(values) if values else 0.0

        result = {
            'epsilon': epsilon,
            'alpha': alpha,
            'attack_type': attack_type,
            'aggregator': aggregator,
            'num_rounds': base_config.NUM_ROUNDS,
            'total_time': total_exp_time,
            'avg_round_latency': total_exp_time / base_config.NUM_ROUNDS,
            **final_metrics,
            'run_id': run_id
        }

        print(f"  âœ“ Finished: Îµ={epsilon}, Î±={alpha}, att={attack_type}, agg={aggregator} -> F1={result.get('final_f1', 0.0):.4f}")
        return result

    def run_full_experiment(self, X_train, y_train, X_test, y_test, input_dim: int):
        """Run full experimental grid in parallel"""

        print("\n" + "="*80)
        print("STARTING FULL EXPERIMENTAL GRID (IN PARALLEL WITH RAY)")
        print("="*80)

        # --- 1. Initialize Ray ---
        if ray.is_initialized():
            ray.shutdown()
        ray.init(num_cpus=os.cpu_count(), log_to_driver=False, ignore_reinit_error=True)
        print(f"\nRay initialized on {os.cpu_count()} CPUs. ðŸ”¥\n")

        # --- 2. Put shared data into Ray's object store ---
        X_train_ref = ray.put(X_train)
        y_train_ref = ray.put(y_train)
        X_test_ref = ray.put(X_test)
        y_test_ref = ray.put(y_test)

        # --- 3. Generate all experiment task configurations ---
        tasks = []
        for run_id in range(self.config.NUM_RUNS):
            for epsilon in self.config.EPSILON_VALUES:
                for alpha in self.config.MALICIOUS_FRACTIONS:
                    for attack_type in self.config.ATTACK_TYPES:
                        if alpha == 0.0 and attack_type != 'none':
                            continue
                        if alpha > 0.0 and attack_type == 'none':
                            continue

                        for aggregator in self.config.AGGREGATORS:
                            tasks.append(
                                ExperimentRunner._run_single_experiment.remote(
                                    X_train_ref, y_train_ref, X_test_ref, y_test_ref, input_dim,
                                    epsilon, alpha, attack_type, aggregator, run_id,
                                    self.config
                                )
                            )

        total_experiments = len(tasks)
        print(f"Total experiments to run: {total_experiments}")
        print("Submitting all tasks to Ray... This will now run in parallel.\n")

        # --- 4. Retrieve all results ---
        all_results = ray.get(tasks)

        print(f"\n{'='*80}")
        print(f"COMPLETED {len(all_results)} EXPERIMENTS")
        print(f"{'='*80}\n")

        ray.shutdown()

        # --- 5. Process results into your original format ---
        for result in all_results:
            if result:
                key = f"eps{result['epsilon']}_alpha{result['alpha']}_{result['attack_type']}_{result['aggregator']}"
                self.results[key].append(result)

        return self.results

    def save_results(self, filename: str = None):
        """Save results to JSON"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{self.config.RESULTS_DIR}/results_{timestamp}.json"

        results_dict = {k: v for k, v in self.results.items()}

        with open(filename, 'w') as f:
            json.dump(results_dict, f, indent=2, default=str)

        print(f"âœ“ Results saved to: {filename}")
        return filename


# ============================================================================
# VISUALIZATION AND ANALYSIS
# ============================================================================

class ResultsAnalyzer:
    """Analyze and visualize experimental results"""

    def __init__(self, results: Dict):
        self.results = results
        self.df = self._results_to_dataframe()

    def _results_to_dataframe(self) -> pd.DataFrame:
        """Convert results dict to DataFrame"""
        rows = []
        for key, experiments in self.results.items():
            for exp in experiments:
                rows.append(exp)
        return pd.DataFrame(rows)

    def plot_rq1_privacy_utility(self, save_path: str = None):
        """RQ1: Privacy vs Utility trade-off"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('RQ1: Privacy-Utility Trade-off', fontsize=16, fontweight='bold')

        df_clean = self.df[(self.df['alpha'] == 0.0) &
                           (self.df['attack_type'] == 'none') &
                           (self.df['aggregator'] == 'fedavg')]

        if df_clean.empty:
            print("âš ï¸ No clean data for RQ1 plot")
            plt.close(fig)
            return

        grouped = df_clean.groupby('epsilon').agg({
            'final_f1': ['mean', 'std'],
            'final_accuracy': ['mean', 'std'],
            'final_auc': ['mean', 'std'],
            'avg_round_latency': ['mean', 'std']
        }).reset_index()

        # F1 vs Epsilon
        ax = axes[0, 0]
        epsilon_vals = grouped['epsilon'].values
        f1_means = grouped[('final_f1', 'mean')].values
        f1_stds = grouped[('final_f1', 'std')].values

        ax.errorbar(epsilon_vals, f1_means, yerr=f1_stds, marker='o', linewidth=2,
                    markersize=8, capsize=5, label='F1 Score')
        ax.set_xlabel('Privacy Budget (Îµ)', fontsize=12)
        ax.set_ylabel('F1 Score', fontsize=12)
        ax.set_title('F1 Score vs Privacy Budget', fontweight='bold')
        ax.grid(True, alpha=0.3)
        ax.set_xscale('log')

        # Accuracy vs Epsilon
        ax = axes[0, 1]
        acc_means = grouped[('final_accuracy', 'mean')].values
        acc_stds = grouped[('final_accuracy', 'std')].values

        ax.errorbar(epsilon_vals, acc_means, yerr=acc_stds, marker='s', linewidth=2,
                    markersize=8, capsize=5, color='green', label='Accuracy')
        ax.set_xlabel('Privacy Budget (Îµ)', fontsize=12)
        ax.set_ylabel('Accuracy', fontsize=12)
        ax.set_title('Accuracy vs Privacy Budget', fontweight='bold')
        ax.grid(True, alpha=0.3)
        ax.set_xscale('log')

        # AUC vs Epsilon
        ax = axes[1, 0]
        auc_means = grouped[('final_auc', 'mean')].values
        auc_stds = grouped[('final_auc', 'std')].values

        ax.errorbar(epsilon_vals, auc_means, yerr=auc_stds, marker='^', linewidth=2,
                    markersize=8, capsize=5, color='red', label='AUC')
        ax.set_xlabel('Privacy Budget (Îµ)', fontsize=12)
        ax.set_ylabel('AUC', fontsize=12)
        ax.set_title('AUC vs Privacy Budget', fontweight='bold')
        ax.grid(True, alpha=0.3)
        ax.set_xscale('log')

        # Latency vs Epsilon
        ax = axes[1, 1]
        lat_means = grouped[('avg_round_latency', 'mean')].values
        lat_stds = grouped[('avg_round_latency', 'std')].values

        ax.errorbar(epsilon_vals, lat_means, yerr=lat_stds, marker='D', linewidth=2,
                    markersize=8, capsize=5, color='purple', label='Latency')
        ax.set_xlabel('Privacy Budget (Îµ)', fontsize=12)
        ax.set_ylabel('Avg Round Latency (s)', fontsize=12)
        ax.set_title('Latency vs Privacy Budget', fontweight='bold')
        ax.grid(True, alpha=0.3)
        ax.set_xscale('log')

        plt.tight_layout(rect=[0, 0.03, 1, 0.95])

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ“ Saved: {save_path}")

        plt.show()

    def plot_rq2_robustness(self, save_path: str = None):
        """RQ2: Robustness under attacks"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('RQ2: Robustness Under Attacks', fontsize=16, fontweight='bold')

        df_fixed_eps = self.df[self.df['epsilon'] == 5.0]

        if df_fixed_eps.empty:
            print("âš ï¸ No data with Îµ=5.0 for RQ2 plot. Trying with another epsilon.")
            if not self.df.empty:
                any_eps = self.df['epsilon'].unique()[0]
                df_fixed_eps = self.df[self.df['epsilon'] == any_eps]
                print(f"Using Îµ={any_eps} instead.")
            else:
                print("No data at all.")
                plt.close(fig)
                return

        # F1 vs Malicious Fraction (by aggregator)
        ax = axes[0, 0]
        for agg in self.df['aggregator'].unique():
            df_agg = df_fixed_eps[(df_fixed_eps['aggregator'] == agg) &
                                  (df_fixed_eps['attack_type'] == 'label_flip')]
            if not df_agg.empty:
                grouped = df_agg.groupby('alpha')['final_f1'].agg(['mean', 'std']).reset_index()
                ax.errorbar(grouped['alpha'], grouped['mean'], yerr=grouped['std'],
                           marker='o', label=agg.capitalize(), linewidth=2, markersize=8, capsize=5)

        ax.set_xlabel('Malicious Client Fraction (Î±)', fontsize=12)
        ax.set_ylabel('F1 Score', fontsize=12)
        ax.set_title('F1 vs Malicious Fraction (Label Flip)', fontweight='bold')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # F1 vs Attack Type (Î±=0.2)
        ax = axes[0, 1]
        df_alpha02 = df_fixed_eps[(df_fixed_eps['alpha'] == 0.2) &
                                  (df_fixed_eps['aggregator'] == 'trimmed_mean')]
        if not df_alpha02.empty:
            grouped = df_alpha02.groupby('attack_type')['final_f1'].agg(['mean', 'std']).reset_index()
            ax.bar(grouped['attack_type'], grouped['mean'], yerr=grouped['std'],
                   capsize=5, alpha=0.7, color=['green', 'orange', 'red', 'purple'])
            ax.set_xlabel('Attack Type', fontsize=12)
            ax.set_ylabel('F1 Score', fontsize=12)
            ax.set_title('F1 vs Attack Type (Î±=0.2, Trimmed Mean)', fontweight='bold')
            ax.grid(True, alpha=0.3, axis='y')
            plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')

        # AUC vs Malicious Fraction (by aggregator)
        ax = axes[1, 0]
        for agg in self.df['aggregator'].unique():
            df_agg = df_fixed_eps[(df_fixed_eps['aggregator'] == agg) &
                                  (df_fixed_eps['attack_type'] == 'label_flip')]
            if not df_agg.empty:
                grouped = df_agg.groupby('alpha')['final_auc'].agg(['mean', 'std']).reset_index()
                ax.errorbar(grouped['alpha'], grouped['mean'], yerr=grouped['std'],
                           marker='s', label=agg.capitalize(), linewidth=2, markersize=8, capsize=5)

        ax.set_xlabel('Malicious Client Fraction (Î±)', fontsize=12)
        ax.set_ylabel('AUC', fontsize=12)
        ax.set_title('AUC vs Malicious Fraction (Label Flip)', fontweight='bold')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # Heatmap: Aggregator vs Attack Type
        ax = axes[1, 1]
        df_heatmap = df_fixed_eps[df_fixed_eps['alpha'] == 0.2]
        pivot = df_heatmap.pivot_table(values='final_f1',
                                       index='aggregator',
                                       columns='attack_type',
                                       aggfunc='mean')

        if not pivot.empty:
            sns.heatmap(pivot, annot=True, fmt='.3f', cmap='RdYlGn',
                        vmin=0, vmax=1, ax=ax, cbar_kws={'label': 'F1 Score'})
            ax.set_title('F1 Score: Aggregator vs Attack (Î±=0.2)', fontweight='bold')
            ax.set_xlabel('Attack Type', fontsize=12)
            ax.set_ylabel('Aggregator', fontsize=12)

        plt.tight_layout(rect=[0, 0.03, 1, 0.95])

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ“ Saved: {save_path}")

        plt.show()

    def plot_rq3_operational_costs(self, save_path: str = None):
        """RQ3: Operational costs"""
        fig, axes = plt.subplots(1, 2, figsize=(15, 5))
        fig.suptitle('RQ3: Operational Costs', fontsize=16, fontweight='bold')

        df_clean = self.df[(self.df['alpha'] == 0.0) & (self.df['attack_type'] == 'none')]

        if df_clean.empty:
            print("âš ï¸ No clean data for RQ3 plot")
            plt.close(fig)
            return

        # Latency vs Epsilon (by aggregator)
        ax = axes[0]
        for agg in df_clean['aggregator'].unique():
            df_agg = df_clean[df_clean['aggregator'] == agg]
            grouped = df_agg.groupby('epsilon')['avg_round_latency'].agg(['mean', 'std']).reset_index()
            ax.errorbar(grouped['epsilon'], grouped['mean'], yerr=grouped['std'],
                           marker='o', label=agg.capitalize(), linewidth=2, markersize=8, capsize=5)

        ax.set_xlabel('Privacy Budget (Îµ)', fontsize=12)
        ax.set_ylabel('Avg Round Latency (s)', fontsize=12)
        ax.set_title('Latency vs Privacy Budget', fontweight='bold')
        ax.set_xscale('log')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # Latency by Aggregator (bar chart)
        ax = axes[1]
        df_eps5 = self.df[(self.df['epsilon'] == 5.0) &
                          (self.df['alpha'] == 0.0) &
                          (self.df['attack_type'] == 'none')]

        if df_eps5.empty:
            print("âš ï¸ No data with Îµ=5.0 for RQ3 bar plot.")
        else:
            grouped = df_eps5.groupby('aggregator')['avg_round_latency'].agg(['mean', 'std']).reset_index()
            ax.bar(grouped['aggregator'], grouped['mean'], yerr=grouped['std'],
                   capsize=5, alpha=0.7, color=['blue', 'orange', 'green', 'red'])

        ax.set_xlabel('Aggregator', fontsize=12)
        ax.set_ylabel('Avg Round Latency (s)', fontsize=12)
        ax.set_title('Latency by Aggregator (Îµ=5.0)', fontweight='bold')
        ax.grid(True, alpha=0.3, axis='y')
        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')

        plt.tight_layout(rect=[0, 0.03, 1, 0.95])

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ“ Saved: {save_path}")

        plt.show()

    def plot_rq4_pareto_frontier(self, save_path: str = None):
        """RQ4: Multi-objective trade-off"""
        fig = plt.figure(figsize=(18, 5))
        gs = fig.add_gridspec(1, 3, hspace=0.3, wspace=0.3)

        fig.suptitle('RQ4: Multi-Objective Trade-off Analysis', fontsize=16, fontweight='bold')

        # 1. F1 vs Epsilon (colored by alpha)
        ax = fig.add_subplot(gs[0, 0])
        df_trimmed = self.df[(self.df['aggregator'] == 'trimmed_mean') &
                             (self.df['attack_type'].isin(['none', 'label_flip']))]

        if df_trimmed.empty:
            print("âš ï¸ No data for RQ4 plot 1")

        for alpha in sorted(df_trimmed['alpha'].unique()):
            df_alpha = df_trimmed[df_trimmed['alpha'] == alpha]
            grouped = df_alpha.groupby('epsilon')['final_f1'].mean().reset_index()
            ax.plot(grouped['epsilon'], grouped['final_f1'],
                    marker='o', linewidth=2, markersize=8, label=f'Î±={alpha}')

        ax.set_xlabel('Privacy Budget (Îµ)', fontsize=12)
        ax.set_ylabel('F1 Score', fontsize=12)
        ax.set_title('F1 vs Îµ (by Malicious Fraction)', fontweight='bold')
        ax.set_xscale('log')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # 2. F1 vs Latency (Pareto frontier)
        ax = fig.add_subplot(gs[0, 1])
        df_pareto = self.df[self.df['attack_type'] == 'none']

        if df_pareto.empty:
            print("âš ï¸ No data for RQ4 plot 2")
        else:
            scatter = ax.scatter(df_pareto['avg_round_latency'],
                                 df_pareto['final_f1'],
                                 c=np.log10(df_pareto['epsilon'].replace(np.inf, 100)),
                                 s=100, alpha=0.6, cmap='viridis', edgecolors='black')
            cbar = plt.colorbar(scatter, ax=ax)
            cbar.set_label('logâ‚â‚€(Îµ)', fontsize=10)

        ax.set_xlabel('Avg Round Latency (s)', fontsize=12)
        ax.set_ylabel('F1 Score', fontsize=12)
        ax.set_title('F1 vs Latency (colored by log(Îµ))', fontweight='bold')
        ax.grid(True, alpha=0.3)

        # 3. 3D scatter: F1 vs Epsilon vs Alpha
        ax = fig.add_subplot(gs[0, 2], projection='3d')
        df_3d = self.df[(self.df['aggregator'] == 'trimmed_mean') &
                        (self.df['attack_type'] == 'label_flip')]

        if df_3d.empty:
            print("âš ï¸ No data for RQ4 plot 3")
        else:
            scatter = ax.scatter(np.log10(df_3d['epsilon'].replace(np.inf, 100)),
                                 df_3d['alpha'],
                                 df_3d['final_f1'],
                                 c=df_3d['final_f1'],
                                 s=100, alpha=0.6, cmap='RdYlGn', edgecolors='black')
            plt.colorbar(scatter, ax=ax, shrink=0.5, label='F1 Score')

        ax.set_xlabel('logâ‚â‚€(Îµ)', fontsize=10)
        ax.set_ylabel('Î± (Malicious Fraction)', fontsize=10)
        ax.set_zlabel('F1 Score', fontsize=10)
        ax.set_title('3D Trade-off Space', fontweight='bold')

        plt.tight_layout(rect=[0, 0.03, 1, 0.95])

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ“ Saved: {save_path}")

        plt.show()

    def generate_traffic_light_table(self, save_path: str = None):
        """Generate operator decision table"""
        print("\n" + "="*80)
        print("TRAFFIC LIGHT DECISION TABLE FOR OPERATORS")
        print("="*80 + "\n")

        # Define thresholds
        f1_green = 0.85
        f1_yellow = 0.75
        latency_green = 5.0
        latency_yellow = 10.0

        if self.df.empty:
            print("No results to analyze.")
            return

        # Group by configuration
        configs = self.df.groupby(['epsilon', 'alpha', 'aggregator', 'attack_type']).agg({
            'final_f1': 'mean',
            'avg_round_latency': 'mean'
        }).reset_index()

        # Assign traffic light
        def get_status(f1, latency):
            if f1 >= f1_green and latency <= latency_green:
                return 'âœ… GREEN'
            elif f1 >= f1_yellow and latency <= latency_yellow:
                return 'âš ï¸ YELLOW'
            else:
                return 'âŒ RED'

        configs['status'] = configs.apply(
            lambda row: get_status(row['final_f1'], row['avg_round_latency']), axis=1
        )

        # Print table
        print(f"{'Îµ':<8} {'Î±':<6} {'Aggregator':<15} {'Attack':<15} {'F1':<8} {'Latency':<10} {'Status':<12}")
        print("-" * 90)

        for _, row in configs.sort_values('final_f1', ascending=False).head(20).iterrows():
            eps_str = f"{row['epsilon']:.1f}" if row['epsilon'] != float('inf') else "âˆž"
            print(f"{eps_str:<8} {row['alpha']:<6.1f} {row['aggregator']:<15} "
                  f"{row['attack_type']:<15} {row['final_f1']:<8.3f} "
                  f"{row['avg_round_latency']:<10.2f} {row['status']:<12}")

        if save_path:
            configs.to_csv(save_path, index=False)
            print(f"\nâœ“ Table saved to: {save_path}")

    def generate_summary_statistics(self):
        """Generate summary statistics for paper"""
        print("\n" + "="*80)
        print("SUMMARY STATISTICS FOR PAPER")
        print("="*80 + "\n")

        if self.df.empty:
            print("No results to analyze.")
            return

        # Best configuration
        best_config = self.df.loc[self.df['final_f1'].idxmax()]
        print("ðŸ† BEST CONFIGURATION:")
        print(f"  Îµ = {best_config['epsilon']}")
        print(f"  Î± = {best_config['alpha']}")
        print(f"  Aggregator = {best_config['aggregator']}")
        print(f"  Attack = {best_config['attack_type']}")
        print(f"  F1 = {best_config['final_f1']:.4f}")
        print(f"  AUC = {best_config['final_auc']:.4f}")
        print(f"  Latency = {best_config['avg_round_latency']:.2f}s")

        # Privacy-utility trade-off
        print("\nðŸ“Š PRIVACY-UTILITY ANALYSIS (Î±=0, FedAvg):")
        for eps in [1.0, 5.0, 10.0]:
            df_eps = self.df[(self.df['epsilon'] == eps) &
                             (self.df['alpha'] == 0.0) &
                             (self.df['attack_type'] == 'none') &
                             (self.df['aggregator'] == 'fedavg')]
            if not df_eps.empty:
                print(f"  Îµ={eps}: F1={df_eps['final_f1'].mean():.4f} Â± {df_eps['final_f1'].std():.4f}")

        # Robustness analysis
        print("\nðŸ›¡ï¸ ROBUSTNESS ANALYSIS (Îµ=5.0, Î±=0.2, Label Flip):")
        for agg in ['fedavg', 'trimmed_mean', 'median', 'krum']:
            df_agg = self.df[(self.df['epsilon'] == 5.0) &
                             (self.df['alpha'] == 0.2) &
                             (self.df['attack_type'] == 'label_flip') &
                             (self.df['aggregator'] == agg)]
            if not df_agg.empty:
                print(f"  {agg.capitalize()}: F1={df_agg['final_f1'].mean():.4f} Â± {df_agg['final_f1'].std():.4f}")

        # Operational costs
        print("\nâš¡ OPERATIONAL COSTS (Î±=0, FedAvg):")
        df_ops = self.df[(self.df['alpha'] == 0.0) & (self.df['attack_type'] == 'none') & (self.df['aggregator'] == 'fedavg')]
        if not df_ops.empty:
            print(f"  Avg latency (vs Îµ): {df_ops.groupby('epsilon')['avg_round_latency'].mean().to_dict()}")
        else:
            print("  No data for operational cost summary.")


# ============================================================================
# MAIN EXECUTION
# ============================================================================

def main():
    """Main execution function"""

    print("\n" + "="*80)
    print("Q1-QUALITY NWDAF FL+DP EXPERIMENT")
    print("Dataset: UNSW-NB15")
    print("="*80 + "\n")

    try:
        # 1. Load and preprocess data
        print("STEP 1: Loading data...")
        data_loader = UNSWDataLoader(config.DATA_PATH)
        X, y, feature_names = data_loader.load_and_preprocess()

        # 2. Split data
        print("\nSTEP 2: Splitting data...")
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=config.TEST_SIZE, random_state=config.RANDOM_SEED, stratify=y
        )

        # Normalize
        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train)
        X_test = scaler.transform(X_test)

        print(f"âœ“ Training set: {X_train.shape}")
        print(f"âœ“ Test set: {X_test.shape}")

        input_dim = X_train.shape[1]

        # 3. Run experiments
        print("\nSTEP 3: Running experiments...")
        runner = ExperimentRunner(config)
        results = runner.run_full_experiment(X_train, y_train, X_test, y_test, input_dim)

        # 4. Save results
        print("\nSTEP 4: Saving results...")
        results_file = runner.save_results()

        # 5. Analyze and visualize
        print("\nSTEP 5: Analyzing results...")
        analyzer = ResultsAnalyzer(results)

        # Generate plots
        analyzer.plot_rq1_privacy_utility(
            save_path=f"{config.RESULTS_DIR}/rq1_privacy_utility.png"
        )

        analyzer.plot_rq2_robustness(
            save_path=f"{config.RESULTS_DIR}/rq2_robustness.png"
        )

        analyzer.plot_rq3_operational_costs(
            save_path=f"{config.RESULTS_DIR}/rq3_costs.png"
        )

        analyzer.plot_rq4_pareto_frontier(
            save_path=f"{config.RESULTS_DIR}/rq4_pareto.png"
        )

        # Generate tables
        analyzer.generate_traffic_light_table(
            save_path=f"{config.RESULTS_DIR}/traffic_light_table.csv"
        )

        analyzer.generate_summary_statistics()

        print("\n" + "="*88)
        print("âœ… EXPERIMENT COMPLETE!")
        print("="*88)
        print(f"\nResults saved in: {config.RESULTS_DIR}/")
        print("Files generated:")
        print(f"  - {os.path.basename(results_file)}")
        print(f"  - rq1_privacy_utility.png")
        print(f"  - rq2_robustness.png")
        print(f"  - rq3_costs.png")
        print(f"  - rq4_pareto.png")
        print(f"  - traffic_light_table.csv")
        print("\nðŸ“Š Use these results for your Q1 paper!")

    except FileNotFoundError as e:
        print(f"\nâŒ CRITICAL ERROR: Could not find data file.")
        print(f"  Details: {e}")
        print(f"  Please check your DATA_PATH in the ExperimentConfig class.")
    except Exception as e:
        print(f"\nâŒ AN UNEXPECTED ERROR OCCURRED: {e}")
        import traceback
        traceback.print_exc()

# ============================================================================
# RUN
# ============================================================================

if __name__ == "__main__":
    # In a Colab notebook, you will call main() in a separate cell.
    # If running as a .py file, this will execute.

    # We call main() directly for running in a script
    main()